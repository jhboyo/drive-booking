"""
ì‹¤í—˜ 1: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜

í˜„ì¬ ì„¤ì • vs ê°œì„  ì„¤ì • ë¹„êµ:
- Phase 1 ì‚¬ì „í•™ìŠµ: 300 â†’ 1000 ì—í”¼ì†Œë“œ
- Phase 2 ì‚¬ì „í•™ìŠµ: 300 â†’ 1000 ì—í”¼ì†Œë“œ
- í†µí•© í•™ìŠµ: 500 â†’ 1000 ì—í”¼ì†Œë“œ
- DQN Hidden Dim: 128 â†’ 256
"""

import json
import sys
import time
from pathlib import Path

import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.integrated_system import IntegratedSystem, train_integrated, evaluate_integrated
from src.agents.q_learning_agent import QLearningAgent
from src.agents.scheduling_agent import DQNAgent


def run_baseline_config(seed: int = 42) -> dict:
    """
    ê¸°ì¡´ ì„¤ì •ìœ¼ë¡œ ì‹¤í—˜ (Baseline)

    ì„¤ì •:
        - Phase 1 ì‚¬ì „í•™ìŠµ: 300 ì—í”¼ì†Œë“œ
        - Phase 2 ì‚¬ì „í•™ìŠµ: 300 ì—í”¼ì†Œë“œ
        - í†µí•© í•™ìŠµ: 500 ì—í”¼ì†Œë“œ
        - DQN Hidden Dim: 128
    """
    print("\n" + "=" * 60)
    print("ğŸ”µ Baseline ì„¤ì • ì‹¤í—˜")
    print("=" * 60)
    print("Phase 1 ì‚¬ì „í•™ìŠµ: 300 ì—í”¼ì†Œë“œ")
    print("Phase 2 ì‚¬ì „í•™ìŠµ: 300 ì—í”¼ì†Œë“œ")
    print("í†µí•© í•™ìŠµ: 500 ì—í”¼ì†Œë“œ")
    print("DQN Hidden Dim: 128")
    print("=" * 60)

    start_time = time.time()

    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í†µí•© ì‹œìŠ¤í…œ ìƒì„±
    system = IntegratedSystem(seed=seed)

    # í•™ìŠµ ì‹¤í–‰
    history = train_integrated(
        system=system,
        phase1_pretrain=300,
        phase2_pretrain=300,
        n_episodes=500,
        verbose=True
    )

    # í‰ê°€ ì‹¤í–‰
    results = evaluate_integrated(
        system=system,
        n_episodes=100,
        verbose=False
    )

    elapsed = time.time() - start_time
    results['training_time'] = elapsed
    results['config'] = 'baseline'

    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")

    return results


def run_tuned_config(seed: int = 42) -> dict:
    """
    íŠœë‹ëœ ì„¤ì •ìœ¼ë¡œ ì‹¤í—˜

    ì„¤ì •:
        - Phase 1 ì‚¬ì „í•™ìŠµ: 1000 ì—í”¼ì†Œë“œ
        - Phase 2 ì‚¬ì „í•™ìŠµ: 1000 ì—í”¼ì†Œë“œ
        - í†µí•© í•™ìŠµ: 1000 ì—í”¼ì†Œë“œ
        - DQN Hidden Dim: 256
    """
    print("\n" + "=" * 60)
    print("ğŸŸ¢ íŠœë‹ëœ ì„¤ì • ì‹¤í—˜")
    print("=" * 60)
    print("Phase 1 ì‚¬ì „í•™ìŠµ: 1000 ì—í”¼ì†Œë“œ")
    print("Phase 2 ì‚¬ì „í•™ìŠµ: 1000 ì—í”¼ì†Œë“œ")
    print("í†µí•© í•™ìŠµ: 1000 ì—í”¼ì†Œë“œ")
    print("DQN Hidden Dim: 256")
    print("=" * 60)

    start_time = time.time()

    # íŠœë‹ëœ DQN ì—ì´ì „íŠ¸ ìƒì„± (Hidden Dim 256)
    tuned_phase2_agent = DQNAgent(
        state_dim=159,
        action_dim=6,
        hidden_dim=256,  # 128 â†’ 256ìœ¼ë¡œ ì¦ê°€
        learning_rate=1e-3,
        discount_factor=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        buffer_size=10000,
        batch_size=64,
        target_update_freq=10,
        seed=seed
    )

    # í†µí•© ì‹œìŠ¤í…œ ìƒì„± (íŠœë‹ëœ ì—ì´ì „íŠ¸ ì‚¬ìš©)
    system = IntegratedSystem(
        phase2_agent=tuned_phase2_agent,
        seed=seed
    )

    # í•™ìŠµ ì‹¤í–‰ (ì¦ê°€ëœ ì—í”¼ì†Œë“œ)
    history = train_integrated(
        system=system,
        phase1_pretrain=1000,   # 300 â†’ 1000
        phase2_pretrain=1000,   # 300 â†’ 1000
        n_episodes=1000,        # 500 â†’ 1000
        verbose=True
    )

    # í‰ê°€ ì‹¤í–‰
    results = evaluate_integrated(
        system=system,
        n_episodes=100,
        verbose=False
    )

    elapsed = time.time() - start_time
    results['training_time'] = elapsed
    results['config'] = 'tuned'

    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")

    return results


def compare_results(baseline: dict, tuned: dict) -> dict:
    """
    ë‘ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¹„êµ")
    print("=" * 60)

    # evaluate_integrated ë°˜í™˜ê°’ê³¼ ì¼ì¹˜í•˜ëŠ” í‚¤ ì´ë¦„ ì‚¬ìš©
    metrics = [
        ('mean_total_reward', 'ì´ ë³´ìƒ'),
        ('end_to_end_success_rate', 'End-to-End ì„±ê³µë¥ '),
        ('preferred_time_match_rate', 'ì„ í˜¸ ì‹œê°„ ë§¤ì¹­ë¥ '),
        ('mean_synergy_bonus', 'ì‹œë„ˆì§€ ë³´ë„ˆìŠ¤'),
        ('mean_questions', 'í‰ê·  ì§ˆë¬¸ ìˆ˜'),
        ('mean_attempts', 'í‰ê·  ìŠ¤ì¼€ì¤„ë§ ì‹œë„'),
    ]

    comparison = {}

    print(f"\n{'ì§€í‘œ':<25} {'Baseline':>12} {'Tuned':>12} {'ê°œì„ ':>12}")
    print("-" * 65)

    for key, name in metrics:
        baseline_val = baseline.get(key, 0)
        tuned_val = tuned.get(key, 0)

        if key in ['end_to_end_success_rate', 'preferred_time_match_rate']:
            # ë°±ë¶„ìœ¨ë¡œ í‘œì‹œ
            baseline_str = f"{baseline_val * 100:.1f}%"
            tuned_str = f"{tuned_val * 100:.1f}%"
            diff = (tuned_val - baseline_val) * 100
            diff_str = f"{diff:+.1f}%p"
        else:
            baseline_str = f"{baseline_val:.2f}"
            tuned_str = f"{tuned_val:.2f}"
            if baseline_val != 0:
                diff = ((tuned_val - baseline_val) / abs(baseline_val)) * 100
                diff_str = f"{diff:+.1f}%"
            else:
                diff_str = "N/A"

        print(f"{name:<25} {baseline_str:>12} {tuned_str:>12} {diff_str:>12}")

        comparison[key] = {
            'baseline': baseline_val,
            'tuned': tuned_val,
            'improvement': tuned_val - baseline_val
        }

    # í•™ìŠµ ì‹œê°„ ë¹„êµ
    print("-" * 65)
    print(f"{'í•™ìŠµ ì‹œê°„':<25} {baseline['training_time']:>10.1f}s {tuned['training_time']:>10.1f}s")

    return comparison


def main():
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜ ë©”ì¸ í•¨ìˆ˜
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª ì‹¤í—˜ 1: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("=" * 60)

    seed = 42

    # 1. Baseline ì‹¤í—˜
    baseline_results = run_baseline_config(seed=seed)

    # 2. íŠœë‹ëœ ì„¤ì • ì‹¤í—˜
    tuned_results = run_tuned_config(seed=seed)

    # 3. ê²°ê³¼ ë¹„êµ
    comparison = compare_results(baseline_results, tuned_results)

    # 4. ê²°ê³¼ ì €ì¥
    results_dir = project_root / "results" / "experiments"
    results_dir.mkdir(parents=True, exist_ok=True)

    output = {
        'experiment': 'hyperparameter_tuning',
        'baseline': baseline_results,
        'tuned': tuned_results,
        'comparison': comparison
    }

    output_path = results_dir / "experiment1_hyperparams.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")

    # 5. ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ìµœì¢… ìš”ì•½")
    print("=" * 60)

    baseline_reward = baseline_results['mean_total_reward']
    tuned_reward = tuned_results['mean_total_reward']
    improvement = ((tuned_reward - baseline_reward) / baseline_reward) * 100

    print(f"Baseline ì´ ë³´ìƒ: {baseline_reward:.2f}")
    print(f"Tuned ì´ ë³´ìƒ: {tuned_reward:.2f}")
    print(f"ê°œì„ ìœ¨: {improvement:+.1f}%")

    if improvement > 0:
        print("\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ì¶”ê°€ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    return output


if __name__ == "__main__":
    results = main()
