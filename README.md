# 강화학습 기반 대화형 차량 추천 및 스케줄링 시스템

**Conversational Vehicle Recommendation and Scheduling System using Reinforcement Learning**


🚀 **[실시간 데모 바로가기](https://driving-booking.streamlit.app/)**
---


## 💡 연구 동기

본 프로젝트는 EMNLP 2018에 발표된 **"Playing 20 Question Game with Policy-Based Reinforcement Learning"** (Hu et al., 2018)[arXiv:1808.07645](https://arxiv.org/abs/1808.07645) 논문에서 영감을 받아, 학술적 게임 환경의 강화학습 기법을 **실제 자동차 업무 도메인에 적용**한 연구임.

#### 원 논문의 핵심 아이디어

| 구분 | 20 Questions 게임 |
|------|-------------------|
| **목표** | 최대 20개 질문으로 사용자가 생각한 물체(유명인, 동물 등) 맞추기 |
| **방법** | 정책 기반 강화학습으로 최적의 질문 선택 정책 학습 |
| **핵심 기여** | 노이즈 있는 답변에 강건, 객체 데이터베이스 의존성 제거 |

#### 본 프로젝트의 확장

원 논문의 "질문을 통한 정보 수집 → 최종 추론" 프레임워크를 **자동차 시승 예약 도메인**에 확장 적용함:

| 구분 | 20 Questions 게임 | 시승 예약 시스템 (본 프로젝트) |
|------|-------------------|-------------------------------|
| **환경** | 게임 환경 | 실제 업무 환경 (차량 DB, 센터 스케줄) |
| **목표** | 물체 맞추기 | 최적 차량 추천 + 일정 배정 |
| **질문 수** | 최대 20개 | **필수 4개 + 선택 4개** (효율성 강조) |
| **보상** | 정답 여부 | 고객 만족도 + 예약 성사 - 질문 수 |
| **확장** | 단일 Phase(파이프라인) | **Two-Phase** (차량 추천 + 일정 스케줄링) |

#### 연구적 기여

1. **도메인 적용**: 학술적 게임 환경 → 실제 비즈니스 프로세스
2. **효율성 강화**: 20개 질문 → **필수 4개 질문 + 선택 4개 질문**으로 목표 달성
3. **파이프라인 확장**: 정보 수집 → 추천 → **스케줄링까지 End-to-End 통합**
4. **시너지 최적화**: Phase 간 협업 효과를 Synergy Bonus로 정량화

---

## 🚨 해결하고자 하는 문제

### 기존 시승 예약 프로세스의 불편함

| 브랜드 | 단계 수 | 주요 단계 |
|--------|---------|-----------|
| **Hyundai** | 7단계 | 모델 → 장소 → 방법 → 일정 → 운전경력 → 보유차종 → 요청사항 |
| **Genesis** | 5단계 | 차량 → 드라이빙라운지 → 일정 → 유의사항 → 확인 |
| **Kia** | 7단계 | 모델 → 거점 → 방법 → 일정 → 동의 → 시승자 → 설문 |

**고객 Pain Points**: 수십 개 모델 중 선택 어려움, 5~7단계 직접 입력, 캘린더에서 가용 시간 직접 확인

### 강화학습 기반 해결책

```
AS-IS: 고객이 모든 것을 직접 선택 (5~7단계)
         ↓
TO-BE: AI가 최소 질문으로 추천 + 최적 일정 자동 배정 (2단계)
```

| Phase | 역할 | 알고리즘 |
|-------|------|----------|
| **차량 추천** | 필수 4개 질문으로 최적 차량 추천 | Q-Learning |
| **스케줄링** | 고객 선호 + 센터 가용성 기반 일정 배정 | DQN |

> 📱 **모바일 앱으로 구현**: Streamlit 기반 반응형 웹앱으로 모바일 환경에서 최적화된 사용자 경험 제공

---

## 🚀 실행 방법

### 온라인 데모

별도 설치 없이 바로 체험 가능: **https://driving-booking.streamlit.app/**

### 로컬 실행

```bash
# uv 설치 (최초 1회)
pip install uv

# 의존성 설치
uv sync

# 대화형 추천 챗봇 실행
uv run streamlit run src/app/main.py
```

브라우저에서 `http://localhost:8501` 접속하여 사용.

---

## 📱 실행 결과 (Mobile App Demo)

### 모바일 최적화 대화형 추천 시스템

학습된 Q-Learning 모델을 **모바일 친화적 UI**로 구현하여 실시간 대화형 추천 시스템을 제공함.

> 🎯 **모바일 앱 특징**: 터치 기반 인터페이스, 반응형 디자인, 실시간 MDP 상태 시각화

<p align="center">
  <img src="./docs/chat/chat1.png" width="260" height="520" style="border: 12px solid #1a1a1a; border-radius: 44px; margin: 10px; box-shadow: 0 20px 60px rgba(0,0,0,0.3), inset 0 0 0 2px #333; object-fit: cover; object-position: left top;" alt="시작 화면"/>
  <img src="./docs/chat/chat2.png" width="260" height="520" style="border: 12px solid #1a1a1a; border-radius: 44px; margin: 10px; box-shadow: 0 20px 60px rgba(0,0,0,0.3), inset 0 0 0 2px #333; object-fit: cover; object-position: top;" alt="질문 단계"/>
</p>
<p align="center">
  <img src="./docs/chat/chat3.png" width="260" height="520" style="border: 12px solid #1a1a1a; border-radius: 44px; margin: 10px; box-shadow: 0 20px 60px rgba(0,0,0,0.3), inset 0 0 0 2px #333; object-fit: cover; object-position: top;" alt="추천 결과"/>
  <img src="./docs/chat/chat4.png" width="260" height="520" style="border: 12px solid #1a1a1a; border-radius: 44px; margin: 10px; box-shadow: 0 20px 60px rgba(0,0,0,0.3), inset 0 0 0 2px #333; object-fit: cover; object-position: top;" alt="예약 완료"/>
</p>

| 단계 | 화면 | 설명 |
|------|------|------|
| **1. 시작** | 좌측 상단 | MDP 상태 시각화 (Action, Reward, State, ε, Step, Policy) |
| **2. 질문** | 우측 상단 | 필수 질문 4개 수집, ε-greedy Policy 표시 |
| **3. 추천** | 좌측 하단 | 고객 응답 기반 최적 차량 추천, 상세 정보 표시 |
| **4. 완료** | 우측 하단 | 지역 기반 시승센터 매칭, 예약 확정 (Reward +15) |

### MDP 시각화 요소

| 요소 | 설명 | 표시 예시 |
|------|------|-----------|
| 🎯 **Action** | 현재 수행 중인 행동 | 용도 질문, 차량 추천, 예약 완료 |
| 🏆 **Reward** | 누적 보상 | +15.0 (예약 성공 시) |
| 📋 **State** | 질문 진행률 | 4/8 (4개 질문 완료) |
| 🎲 **ε** | 탐험률 | 0.05 (학습 후) |
| 👣 **Step** | 에피소드 스텝 | 6 |
| 🔍/🎯 **Policy** | 탐험/활용 모드 | 활용 (Q값 기반 선택) |

### 핵심 성과

| 지표 | 결과 |
|------|------|
| **평균 질문 수** | 4개 (필수) + α (선택) |
| **예약 완료 Reward** | +15.0 |
| **모델 동기화** | 5 에피소드마다 자동 저장 |
| **실시간 학습** | 챗봇 사용 시 Q-table 업데이트 |

---

## 🤖 왜 강화학습인가?

### 대화형 추천의 RL 적합성

본 프로젝트는 **"20 Questions" 게임**과 유사한 구조를 가짐:

```
Agent: "주로 어떤 용도로 사용하세요?"     ← Action 1: 질문 선택
고객: "출퇴근이요"                        ← 환경 응답 (State 변화)
Agent: "가족은 몇 명이세요?"              ← Action 2: 추가 질문 or 추천?
고객: "4명이요"                           ← 환경 응답 (State 변화)
Agent: "싼타페 하이브리드 추천드립니다"    ← Action 3: 추천 결정
고객: "마음에 들어요!" (만족도 80%)        ← Reward 발생
```

### RL이 학습하는 것

| 학습 목표 | 설명 |
|-----------|------|
| **질문 최소화** | 몇 개의 질문으로 정확히 추천할 수 있는지 |
| **질문 순서 최적화** | 어떤 질문이 정보량(Information Gain)이 높은지 |
| **추천 타이밍** | 언제 질문을 멈추고 추천해야 하는지 |
| **Exploration-Exploitation** | 새로운 질문 전략 탐색 vs 검증된 전략 활용 |

### 강화학습 적용의 핵심 요소

| 요소 | 대화형 추천 (본 프로젝트) |
|------|---------------------------|
| Sequential Decision | ✅ 질문 → 질문 → 추천 |
| Delayed Reward | ✅ 추천 후 만족도 확인 |
| State Transition | ✅ 고객 응답으로 정보 축적 |
| Exploration-Exploitation  | ✅ 더 질문할지 vs 바로 추천할지 |

---

## 🎯 MDP 설계

### 차량 추천 (Q-Learning)

**State**
| 구성 요소 | 설명 |
|-----------|------|
| 고객 정보 | 나이, 성별, 외국인, 직장인, 관심차량 유무 |
| 질문 응답 | 8개 질문 × 5개 옵션 (one-hot 인코딩) |
| 질문 횟수 | 현재까지 질문 수 / 최대 질문 수 |
| 차량 점수 | 23종 차량별 매칭 점수 |

**Action (12개)**
| 인덱스 | 액션 | 설명 |
|--------|------|------|
| 0-7 | 질문 선택 | 용도, 연료, 가족, 예산, 우선순위, 크기, 차체, 색상 |
| 8-11 | 차량 추천 | 1순위, 2순위, 3순위, 대체 차량 추천 |

**Reward**
| 상황 | 보상 | 설명 |
|------|------|------|
| 필수 질문 (4개) | 0 | 용도, 연료타입, 가족구성원, 지역 |
| 추가 질문 | -1 | 불필요한 질문 페널티 |
| 다른 차량 보기 | -5 | 추천 실패 페널티 |
| 예약 확정 | **+15** | 성공적인 추천 |

### 스케줄링 (DQN)

**State**
| 구성 요소 | 설명 |
|-----------|------|
| 고객 선호도 | 선호 날짜, 시간대, 유연성 |
| 슬롯 가용성 | 21일 × 6슬롯 (09:00~18:00) |
| 차량 가용성 | 23종 차량별 가용 여부 |
| 메타 정보 | 대기 요청 수, 현재 부하율 등 |

**Action (6개)**
| 인덱스 | 액션 | 설명 |
|--------|------|------|
| 0 | 예약 확정 | 선호 시간에 즉시 예약 |
| 1 | 같은 날 대안 | 같은 날 다른 시간 제안 |
| 2 | 다음 날 대안 | 다음 날 같은 시간 제안 |
| 3 | 평일 대안 | 평일 퇴근 후 시간 제안 |
| 4 | 다음주 대안 | 다음주 같은 요일 제안 |
| 5 | 인센티브 제공 | 비선호 시간대 할인 제공 |

**Reward**
| 상황 | 보상 | 설명 |
|------|------|------|
| 예약 성사 | +10 | 최종 예약 완료 |
| 선호 시간 매칭 | +5 | 1순위 시간 배정 |
| 대안 수락 | +3 | 대안 제시 후 수락 |
| 부하 분산 | +2 | 예약 균등 분포 기여 |
| 고객 거절 | -5 | 고객 이탈 |

---

## 🏗️ 시스템 아키텍처

**고객 접속** → **차량 추천** → **스케줄링** → **예약 확정**

| 단계 | 역할 | 알고리즘 | 입력 → 출력 |
|------|------|----------|-------------|
| **차량 추천 (Phase 1)** | 최소 질문으로 최적 차량 추천 | Q-Learning | 고객 응답 → 추천 차량 |
| **스케줄링 (Phase 2)** | 고객·센터 만족 일정 배정 | DQN | 선호 일정 → 예약 확정 |
| **파이프라인 (Phase 3)** | End-to-End 통합 최적화 | Q-Learning + DQN + Synergy | 고객 접속 → 예약 확정 |

차량 추천 결과가 스케줄링의 입력으로 전달되어, **차량 추천부터 일정 예약까지 End-to-End**로 처리됨. Synergy Bonus를 통해 전체 시스템 최적화를 수행함.

### 통합 보상(Total Reward) 함수

```
Total_Reward = R1 + R2 + Synergy_Bonus

R1 (차량 추천(Phase 1)): 고객 만족도 + 예약 의향 - 질문 수
R2 (스케줄링(Phase 2)): 예약 성사 + 선호 시간 매칭 - 대기시간
Synergy_Bonus: 즉시 예약 가능 차량 추천 시 보너스
```

### 평가 지표

| 단계 | 지표 | 설명 |
|------|------|------|
| 차량 추천 | 추천 정확도 | 고객 선호도와 추천 차량 매칭 점수 |
| 차량 추천 | 질문 효율성 | 평균 질문 수 |
| 스케줄링 | 예약 성사율 | 예약 완료 / 전체 요청 |
| 스케줄링 | 선호 시간 매칭률 | 1순위 시간 배정 비율 |
| 통합 | Total Reward | Episode당 누적 보상 |

### 성능 결과

**학습 조건**: Phase 1 사전학습 1000 에피소드 + Phase 2 사전학습 1000 에피소드 + 통합 학습 1000 에피소드

| 지표 | 기존 | 개선 후 | 변화 |
|------|------|---------|------|
| **총 보상** | 23.84 | **31.98 ± 8.52** | **+34.1%** |
| **End-to-End 성공률** | 83.0% | **89.0%** | +6.0%p |
| **선호 시간 매칭률** | 46.0% | 43.0% | -3.0%p |
| **시너지 보너스** | 4.06 | **10.76** | **+165%** |
| **총 상호작용 횟수** | 2.57회 | 2.57회 | 유지 |

#### 보상 구성 (개선 후)

| 구성 요소 | 평균 보상 | 비율 |
|-----------|-----------|------|
| 차량 추천 보상 | 9.84 | 30.8% |
| 스케줄링 보상 | 11.38 | 35.6% |
| Synergy 보너스 | 10.76 | **33.6%** |

### 개선 사항(파라미터 튜닝)

3가지 개선 방법을 실험하고 **실제 코드에 적용**함:

| 실험 | 방법 | 총 보상 | 개선율 | 적용 |
|------|------|---------|--------|------|
| 기준선 | 기존 Phase 3 | 23.86 | - | - |
| 실험 1 | 하이퍼파라미터 튜닝 | 24.80 | +3.9% | ✅ |
| 실험 2 | 정교한 시너지 보너스(시승 위치 질문 추가) | 30.54 | +28.1% | ✅ |
| **실험 3** | **가용성 정보 활용** | **31.98** | **+34.1%** | ✅ |

**적용된 개선 사항**:
1. **하이퍼파라미터 튜닝**: 에피소드 수 증가 (1000), DQN hidden_dim 256
2. **개선된 시너지 보너스**: 연속적 보상 체계 (최대 17.5점)
3. **가용성 정보 활용**: 차량 추천에서 가용 차량 추천 시 보너스 (+2점)

**최종 성능**: Random 베이스라인 대비 **+120.5%** 개선

---

## 🛠️ 기술 스택

| 분류 | 기술 | 용도 |
|------|------|------|
| **언어** | Python 3.10+ | 전체 시스템 개발 |
| **강화학습** | Gymnasium | RL 환경 구현 |
| **딥러닝** | PyTorch | DQN 신경망 |
| **웹앱** | Streamlit | 모바일 챗봇 UI |
| **데이터** | NumPy, Pandas | 상태 벡터 처리 |
| **시각화** | Matplotlib, Seaborn | 학습 곡선, 성능 분석 |
| **패키지 관리** | uv | 고속 의존성 관리 |

---

## 📖 참고 자료

### 핵심 논문

- Hu, H., Wu, X., Luo, B., Tao, C., Xu, C., Wu, W., & Chen, Z. (2018). **Playing 20 Question Game with Policy-Based Reinforcement Learning**. *EMNLP 2018*. [arXiv:1808.07645](https://arxiv.org/abs/1808.07645)


### 기술 문서

- [Gymnasium Documentation](https://gymnasium.farama.org)
- [Spinning Up in Deep RL](https://spinningup.openai.com)

---

## 👤 Author

- **소속**: 서강대학교 AI SW 대학원
- **학과**: 데이터사이언스 인공지능학과
- **이름**: 김준호
- **학번**: A71023
