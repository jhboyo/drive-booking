# 강화학습 프로젝트: 자동차 시승 예약 통합 시스템

> **Interactive Conversational Recommendation System using Reinforcement Learning**

## 📋 프로젝트 개요

자동차 브랜드 홈페이지의 시승 예약 과정에서 고객이 겪는 번거로움을 **강화학습 기반 대화형 추천 시스템**으로 해결함.

### 강화학습 관점의 문제 정의

본 프로젝트는 **Agent-Environment 상호작용** 구조로 설계됨.

- **Environment**: 고객(응답자), 차량 데이터베이스(재고), 시승 센터 상태(스케줄)로 구성
- **Agent**: 강화학습 기반 의사결정 주체
- **State**: 고객 응답 히스토리, 후보 차량 목록, 센터 가용 상태
- **Action**: 질문 선택, 차량 추천, 일정 배정
- **Reward**: 고객 만족도 + 예약 성사 - 질문 수 - 대기시간

### Two-Phase Hybrid System

본 시스템은 **강화학습 + 규칙 기반 하이브리드** 구조로 설계됨.

- **Phase 1 (Recommendation Agent)**: Q-Learning 기반, 최소 질문으로 최적 차량 추천
- **Phase 2 (Scheduling Agent)**: 하이브리드 방식
  - 고객 응대: 질문 기반 (규칙) - 선호 일정 수집
  - 센터 최적화: DQN 기반 - 자원 배분 및 대안 제시 최적화

두 Phase는 순차적으로 실행되며, Phase 1의 추천 결과가 Phase 2의 입력으로 전달됨.

### 학습 목표

Agent가 학습을 통해 스스로 터득해야 하는 것:

1. 어떤 질문이 고객 선호 파악에 효과적인가?
2. 몇 개의 질문 후 추천해야 정확도와 효율의 균형이 맞는가?
3. 어떤 차량을 추천해야 고객 만족도가 높은가?
4. 어떤 시간대에 배정해야 고객과 센터 모두 만족하는가?

### 차별화 포인트

1. **대화형 추천**: 단순 필터링이 아닌 Sequential Decision Making 기반 추천
2. **Two-Phase 통합**: 추천과 스케줄링을 연결한 실용적 시스템
3. **시뮬레이션 환경**: 도메인 지식 기반 현실적인 고객·센터 시뮬레이션

### 구현 범위

- **분석 대상**: Hyundai, Kia, Genesis 3개 브랜드 시승 예약 프로세스
- **구현 대상**: **Hyundai** 브랜드 기준으로 구현 (7단계 프로세스 → 2단계로 단순화)
- **확장성**: 차량 DB와 질문셋만 교체하면 다른 브랜드에도 적용 가능한 구조로 설계

---

## 🚨 현재 문제

### 브랜드별 시승 예약 프로세스 분석

| 브랜드 | 단계 수 | 주요 단계 |
|--------|---------|-----------|
| **Hyundai** | 7단계 | 모델 → 장소 → 방법 → 일정 → 운전경력 → 보유차종 → 요청사항 |
| **Genesis** | 5단계 | 차량 → 드라이빙라운지 → 일정 → 유의사항 → 확인 |
| **Kia** | 7단계 | 모델 → 거점 → 방법 → 일정 → 동의 → 시승자 → 설문 |

> 📁 실제 화면 캡처: `resource/image/brand/` 참조

### 고객의 번거로움 (Pain Points)

1. **차량 선택의 어려움**: 수십 개 모델 중 어떤 차가 나에게 맞는지 모름
2. **복잡한 단계**: 5~7단계를 모두 직접 입력해야 함
3. **일정 선택의 불편함**: 캘린더에서 가능한 시간대를 직접 확인 필요
4. **정보 입력 반복**: 운전경력, 보유차종 등 매번 입력

### 강화학습으로 해결

```
AS-IS: 고객이 모든 것을 직접 선택 (5~7단계)
         ↓
TO-BE: AI가 최소 질문으로 추천 + 최적 일정 자동 배정 (2단계)
```

- **Phase 1**: 2~3개 질문만으로 최적 시승 차량 추천
- **Phase 2**: 고객 선호 시간 + 센터 가용 상황 고려하여 최적 일정 자동 배정

---

## 🤖 왜 강화학습인가?

### 단순 추천 vs 대화형 추천

| 구분 | 단순 추천 | 대화형 추천 (본 프로젝트) |
|------|-----------|---------------------------|
| 방식 | 프로필 → 즉시 추천 | 질문 → 응답 → 질문 → ... → 추천 |
| 적합 알고리즘 | Collaborative Filtering, Rule-based | **강화학습 (RL)** |
| 의사결정 | 1회성 | **Sequential Decision Making** |

### 대화형 추천의 RL 적합성

본 프로젝트는 **"20 Questions" 게임**과 유사한 구조를 가짐:

```
Agent: "주로 어떤 용도로 사용하세요?"     ← Action 1: 질문 선택
고객: "출퇴근이요"                        ← 환경 응답 (State 변화)
Agent: "가족은 몇 명이세요?"              ← Action 2: 추가 질문 or 추천?
고객: "4명이요"                           ← 환경 응답 (State 변화)
Agent: "싼타페 하이브리드 추천드립니다"    ← Action 3: 추천 결정
고객: "마음에 들어요!" (만족도 80%)        ← Reward 발생
```

### RL이 학습하는 것

| 학습 목표 | 설명 |
|-----------|------|
| **질문 최소화** | 몇 개의 질문으로 정확히 추천할 수 있는지 |
| **질문 순서 최적화** | 어떤 질문이 정보량(Information Gain)이 높은지 |
| **추천 타이밍** | 언제 질문을 멈추고 추천해야 하는지 |
| **Exploration-Exploitation** | 새로운 질문 전략 탐색 vs 검증된 전략 활용 |

### 강화학습 적용의 핵심 요소

| 요소 | 자율주행 | 대화형 추천 (본 프로젝트) |
|------|----------|---------------------------|
| Sequential Decision | ✅ 연속 제어 | ✅ 질문 → 질문 → 추천 |
| Delayed Reward | ✅ 목적지 도착 | ✅ 추천 후 만족도 확인 |
| State Transition | ✅ 차량 위치 변화 | ✅ 고객 응답으로 정보 축적 |
| Exploration-Exploitation | ✅ 새 경로 탐색 | ✅ 더 질문할지 vs 바로 추천할지 |

---

## 🎯 MDP 설계

상세 State, Action, Reward 설계는 [MDP_DESIGN.md](./resource/docs/MDP_DESIGN.md) 참조.

---

## 🏗️ 시스템 아키텍처

```
고객 웹사이트 접속
    ↓
┌─────────────────────────────┐
│  Phase 1: 시승 차량 추천    │
│  - Q-Learning               │
│  - State: 고객 프로필       │
│  - Action: 질문 또는 추천   │
└─────────────────────────────┘
    ↓ (추천 차량 목록)
┌─────────────────────────────┐
│  장소 선택 (규칙 기반)       │
│  - 위치 권한 O → GPS 기반   │
│  - 위치 권한 X → 지역 질문  │
└─────────────────────────────┘
    ↓ (선택된 대리점)
┌─────────────────────────────────────────────┐
│  Phase 2: 스케줄링 (하이브리드)              │
├─────────────────────────────────────────────┤
│  [고객 응대] 규칙 기반                       │
│  - 선호 일정 질문 → 가용성 확인              │
├─────────────────────────────────────────────┤
│  [센터 최적화] DQN 기반                      │
│  - 자원 배분 최적화                          │
│  - 대안 제시 전략 학습                       │
└─────────────────────────────────────────────┘
    ↓
최종 스케줄 + 예약 확정
```

### 단계별 역할

| 단계 | 목적 | 방식 |
|------|------|------|
| **Phase 1** | 최적 차량 추천 | Q-Learning (시뮬레이션 학습) |
| **장소 선택** | 시승 대리점 선택 | 규칙 기반 (GPS 또는 질문) |
| **Phase 2** | 일정 수집 + 센터 최적화 | 하이브리드 (규칙 + DQN) |

### Phase 1 목적 및 구조

Phase 1은 **시뮬레이션 기반 학습**을 통해 최적의 질문 전략을 학습한 모델을 생성하는 것이 목적임.

#### 왜 시뮬레이션인가?

| 방식 | 장점 | 단점 |
|------|------|------|
| 실제 고객으로 학습 | 실제 데이터 | 수천 번 시행착오 → 고객 경험 악화 |
| **시뮬레이션으로 학습** | 빠른 탐색, 무한 반복 가능 | 시뮬레이터 품질에 의존 |

#### Training vs Deployment

```
┌─────────────────────────────────────────────────────────────┐
│  Training Phase (현재 구현)                                  │
├─────────────────────────────────────────────────────────────┤
│  1. 시뮬레이션 고객 생성 (랜덤 프로필: 예산, 가족 수 등)       │
│  2. Agent가 질문 선택 (ε-greedy 탐험)                        │
│  3. Environment가 고객 대신 자동 응답 (숨겨진 선호도 기반)     │
│  4. Agent가 차량 추천                                        │
│  5. Environment가 Reward 자동 계산 (매칭 점수 기반)           │
│  6. Q-table 업데이트 → 1000+ 에피소드 반복                   │
└─────────────────────────────────────────────────────────────┘
                              ↓ 학습된 모델 저장
┌─────────────────────────────────────────────────────────────┐
│  Deployment Phase (추후 구현)                                │
├─────────────────────────────────────────────────────────────┤
│  1. 실제 고객이 질문에 답변                                   │
│  2. 학습된 모델이 최적 질문 선택 (Greedy)                     │
│  3. 학습된 모델이 차량 추천                                   │
│  4. 실제 고객 만족도 피드백 수집                              │
└─────────────────────────────────────────────────────────────┘
```

#### Phase 1 학습 결과

시뮬레이션 학습을 통해 Agent가 스스로 터득하는 것:

- ✅ 어떤 질문 순서가 효율적인가
- ✅ 몇 개의 질문 후 추천해야 하는가
- ✅ 어떤 상황에서 어떤 차량을 추천해야 하는가

### 장소 선택 (규칙 기반)

시승 대리점 선택은 강화학습이 아닌 **규칙 기반**으로 처리함.

```
┌─────────────────────────────────────────────────────┐
│  고객이 웹사이트 접속                                │
└─────────────────────────────────────────────────────┘
                    ↓
        ┌─────────────────────┐
        │  위치 권한 허용?     │
        └─────────────────────┘
           ↓ Yes          ↓ No
    ┌──────────────┐  ┌──────────────────────────┐
    │ GPS 기반      │  │ 질문: "어느 지역에서     │
    │ 주변 대리점   │  │ 시승하시겠어요?"         │
    │ 자동 추천     │  │ (서울/경기/부산/...)     │
    └──────────────┘  └──────────────────────────┘
           ↓                    ↓
        ┌─────────────────────────────────────┐
        │  해당 지역 시승 대리점 목록 제시      │
        │  (거리순 또는 가용 시간 많은 순)     │
        └─────────────────────────────────────┘
```

#### 왜 규칙 기반인가?

- 장소 선택은 **탐색 공간이 작음** (지역 → 대리점)
- 최적화 기준이 **명확함** (거리, 가용성)
- 강화학습의 시행착오가 불필요

### Phase 2 목적 및 구조

Phase 2는 **하이브리드 방식**으로 고객 일정 수집과 센터 자원 최적화를 수행함.

#### 왜 하이브리드인가?

| 구분 | 고객 일정 수집 | 센터 자원 최적화 |
|------|---------------|-----------------|
| 특성 | 고객만 알고 있음 | 복잡한 제약 조건 |
| 최적 방식 | 직접 질문 (규칙) | 학습 기반 (RL) |
| 이유 | 추론 불가능 | 다양한 상황 대응 필요 |

#### Phase 2 전체 흐름

```
┌─────────────────────────────────────────────────────────────┐
│  [1] 고객 응대 (규칙 기반)                                   │
├─────────────────────────────────────────────────────────────┤
│  질문: "언제 시승하시겠어요?"                                │
│  - 선호 날짜 선택 (캘린더 or 이번주/다음주)                   │
│  - 선호 시간대 선택 (오전/오후/저녁)                         │
└─────────────────────────────────────────────────────────────┘
                    ↓ 고객 선호 일정
┌─────────────────────────────────────────────────────────────┐
│  [2] 센터 가용성 확인                                        │
├─────────────────────────────────────────────────────────────┤
│  - 해당 시간대 차량 가용 여부                                │
│  - 담당 직원 배정 가능 여부                                  │
│  - 기존 예약과 충돌 여부                                     │
└─────────────────────────────────────────────────────────────┘
           ↓ 가능                    ↓ 불가능
┌──────────────────────┐  ┌─────────────────────────────────┐
│  예약 확정            │  │  [3] 대안 제시 (DQN 기반)        │
│  - 일정 확정 알림     │  │  - 최적 대안 시간 선택           │
│  - 리마인더 설정      │  │  - 고객 수락률 최대화            │
└──────────────────────┘  └─────────────────────────────────┘
```

#### 센터 자원 및 제약 조건

```
┌─────────────────────────────────────────────────────────────┐
│  시승 센터 자원                                              │
├─────────────────────────────────────────────────────────────┤
│  🚗 시승 차량: 모델별 1~2대 (예: 아반떼 2대, 싼타페 1대)      │
│  👤 담당 직원: 3~5명 (동시 응대 한계)                        │
│  ⏰ 운영 시간: 09:00 ~ 18:00 (1시간 단위 슬롯)               │
│  📅 예약 현황: 일별 예약 밀도 상이                           │
└─────────────────────────────────────────────────────────────┘
```

#### 센터 관점 최적화 (DQN 학습 대상)

| 최적화 목표 | 설명 | Reward |
|-------------|------|--------|
| **대안 제시 전략** | 어떤 대안을 먼저 제시해야 수락률이 높은가 | +수락, -거절 |
| **부하 분산** | 예약이 특정 시간에 몰리지 않게 유도 | +균등 분포 |
| **차량 활용률** | 모든 차량이 골고루 사용되게 | +활용률 |
| **대기열 관리** | 동시 요청 시 우선순위 결정 | +처리량, -대기시간 |

#### 대안 제시 최적화 예시

```
상황: 고객이 "토요일 오전" 원하지만 만석

┌─────────────────────────────────────────────────────────────┐
│  가능한 대안들                                               │
├─────────────────────────────────────────────────────────────┤
│  A. 토요일 오후 2시 (같은 날, 다른 시간)                      │
│  B. 일요일 오전 10시 (다른 날, 같은 시간)                     │
│  C. 금요일 저녁 6시 (평일, 퇴근 후)                          │
│  D. 다음주 토요일 오전 (1주일 후)                            │
└─────────────────────────────────────────────────────────────┘
                    ↓
        DQN이 학습하는 것:
        - 고객 유형별 최적 대안 순서
        - 센터 상황별 추천 전략
        - 수락률을 최대화하는 제안 방식
```

#### Phase 2 MDP 설계

**State (상태)**:
```python
state = {
    'customer_preference': {
        'preferred_date': '2024-01-20',      # 고객 선호 날짜
        'preferred_time': 'morning',          # 오전/오후/저녁
        'flexibility': 'medium'               # 유연성 (high/medium/low)
    },
    'center_state': {
        'available_slots': [...],             # 가용 시간 슬롯
        'vehicle_availability': {...},        # 차량별 가용 현황
        'staff_schedule': {...},              # 직원 스케줄
        'pending_requests': 3                 # 대기 중인 요청 수
    },
    'recommended_car': 'santafe_hybrid'       # Phase 1 추천 차량
}
```

**Action (행동)**:
```python
actions = {
    0: 'confirm_booking',           # 예약 확정
    1: 'suggest_same_day_alt',      # 같은 날 다른 시간 제안
    2: 'suggest_next_day_alt',      # 다음 날 같은 시간 제안
    3: 'suggest_weekday_alt',       # 평일 대안 제안
    4: 'suggest_next_week_alt',     # 다음주 제안
    5: 'offer_incentive'            # 비선호 시간대 인센티브 제공
}
```

**Reward (보상)**:
```python
reward = (
    + 10 * booking_confirmed        # 예약 성사
    + 5 * preferred_time_match      # 선호 시간 매칭
    + 3 * alternative_accepted      # 대안 수락
    - 2 * (waiting_time / 30)       # 대기시간 패널티
    + 2 * load_balance_score        # 부하 분산 점수
    - 5 * customer_rejected         # 고객 거절 (이탈)
)
```

#### Training vs Deployment

```
┌─────────────────────────────────────────────────────────────┐
│  Training Phase (시뮬레이션)                                 │
├─────────────────────────────────────────────────────────────┤
│  1. 시뮬레이션 센터 상태 생성 (가용 차량, 직원, 예약 현황)     │
│  2. 시뮬레이션 고객 요청 생성 (선호 일정, 유연성)             │
│  3. Agent가 예약 확정 또는 대안 제시                         │
│  4. 시뮬레이션 고객이 수락/거절 응답                         │
│  5. Reward 계산 및 DQN 업데이트                              │
└─────────────────────────────────────────────────────────────┘
                              ↓ 학습된 모델 저장
┌─────────────────────────────────────────────────────────────┐
│  Deployment Phase (실제 운영)                                │
├─────────────────────────────────────────────────────────────┤
│  1. 실제 센터 상태 조회 (API 연동)                           │
│  2. 고객이 선호 일정 선택                                    │
│  3. 학습된 모델이 최적 배정 또는 대안 제시                    │
│  4. 예약 확정 및 알림                                        │
└─────────────────────────────────────────────────────────────┘
```

#### Phase 2 학습 결과

시뮬레이션 학습을 통해 Agent가 스스로 터득하는 것:

- ✅ 어떤 대안을 제시해야 고객 수락률이 높은가
- ✅ 센터 자원(차량, 직원)을 효율적으로 배분하는 방법
- ✅ 예약 부하를 균등하게 분산시키는 전략
- ✅ 고객 유형별 최적의 대안 제시 순서

---

## 🔗 통합 시스템 (Phase 1 + Phase 2)

### 통합 시스템 구조

```
고객 입력 → Phase 1 (차량 추천) → Phase 2 (스케줄링) → 예약 확정
```

Phase 1에서 추천된 차량 결과가 Phase 2의 입력으로 전달되어, **차량 추천부터 일정 예약까지 한 번에 처리**하는 End-to-End 파이프라인임.

### 구현 내용

1. **IntegratedSystem 클래스**
   - Phase 1 Agent (Q-Learning)와 Phase 2 Agent (DQN) 연결
   - Phase 1 추천 결과를 Phase 2 입력으로 전달
   - 전체 파이프라인 실행 및 관리

2. **통합 보상 함수**
   ```
   Total_Reward = R1 + R2 + Synergy_Bonus

   R1 (Phase 1): 고객 만족도 + 예약 의향 - 질문 수
   R2 (Phase 2): 예약 성사 + 선호 시간 매칭 - 대기시간
   Synergy_Bonus = 5 × (즉시 예약 가능) + 3 × (추천-스케줄 매칭도)
   ```

3. **End-to-End 평가 메트릭**
   - 전체 예약 성공률 (추천 → 스케줄링 → 확정)
   - 총 상호작용 횟수 (질문 수 + 스케줄링 시도)
   - 고객 만족도 종합 점수
   - 시스템 효율성 (처리 시간, 리소스 활용)

4. **통합 학습 방식**
   - 개별 학습: Phase 1, Phase 2 각각 사전 학습
   - 통합 미세조정: Synergy Bonus로 전체 최적화

### 시너지 효과

| 항목 | 개별 실행 | 통합 실행 |
|------|-----------|-----------|
| 데이터 흐름 | 수동 연결 필요 | 자동 파이프라인 |
| 최적화 | 각 Phase 독립 최적화 | 전체 시스템 최적화 |
| 보상 | R1, R2 개별 계산 | R1 + R2 + Synergy Bonus |
| 추천-스케줄 연계 | 없음 | 즉시 예약 가능 차량 우선 추천 |

---

## 🛠️ 기술 스택

- **Python** 3.12+
- **Gymnasium** (OpenAI Gym)
- **PyTorch** (DQN)
- **NumPy, Pandas, Matplotlib, Seaborn**

---

## 📊 평가 지표

| Phase | 지표 | 설명 |
|-------|------|------|
| Phase 1 | 추천 정확도 | 고객 선호도와 추천 차량 매칭 점수 |
| Phase 1 | 질문 효율성 | 평균 질문 수 |
| Phase 2 | 예약 성사율 | 예약 완료 / 전체 요청 |
| Phase 2 | 대안 수락률 | 대안 수락 / 대안 제시 |
| Phase 2 | 선호 시간 매칭률 | 1순위 시간 배정 비율 |
| Phase 2 | 부하 분산 점수 | 시간대별 예약 균등도 |
| 통합 | Total Reward | Episode당 누적 보상 |

---

## 🚀 실행 방법

### 환경 설정

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Phase 1 학습 (차량 추천)

```bash
# 기본 학습 (1000 에피소드)
python -m src.train_phase1

# 옵션 지정
python -m src.train_phase1 --episodes 2000 --seed 42 --save-model

# 하이퍼파라미터 조정
python -m src.train_phase1 --lr 0.1 --gamma 0.95 --epsilon-decay 0.998
```

| 옵션 | 기본값 | 설명 |
|------|--------|------|
| `--episodes` | 1000 | 학습 에피소드 수 |
| `--seed` | 42 | 랜덤 시드 |
| `--lr` | 0.1 | 학습률 (α) |
| `--gamma` | 0.95 | 할인율 (γ) |
| `--epsilon-decay` | 0.998 | 탐험률 감소율 |
| `--save-model` | False | 모델 저장 여부 |

### 평가

```bash
# 모든 에이전트 평가 (Random, Rule-based, Adaptive, Q-Learning)
python -m src.evaluate

# 저장된 모델 로드하여 평가
python -m src.evaluate --model models/q_learning_model.json

# 평가 에피소드 수 조정
python -m src.evaluate --episodes 200

# 결과를 JSON 파일로 저장
python -m src.evaluate --save-results
```

| 옵션 | 기본값 | 설명 |
|------|--------|------|
| `--episodes` | 100 | 평가 에피소드 수 |
| `--seed` | 42 | 랜덤 시드 |
| `--model` | None | 로드할 Q-Learning 모델 경로 |
| `--train-episodes` | 1000 | 모델 없을 때 학습 에피소드 수 |
| `--save-results` | False | 결과 JSON 저장 여부 |

### Phase 2 학습 (스케줄링)

```bash
# 기본 학습 (500 에피소드)
python -m src.train_phase2

# 옵션 지정
python -m src.train_phase2 --episodes 1000 --seed 42
```

### 통합 시스템 학습 - 예정

```bash
# 통합 시스템 학습
python -m src.train_integrated --episodes 2000

# 통합 평가
python -m src.evaluate_integrated
```

| 옵션 | 기본값 | 설명 |
|------|--------|------|
| `--episodes` | 2000 | 통합 학습 에피소드 수 |
| `--phase1-model` | None | 사전 학습된 Phase 1 모델 |
| `--phase2-model` | None | 사전 학습된 Phase 2 모델 |
| `--save-model` | False | 통합 모델 저장 여부 |

---

## 📅 로드맵

상세 구현 일정은 [ROADMAP.md](./resource/docs/ROADMAP.md) 참조.

---

## 🎯 목표 성과

### Phase 1 (차량 추천)
- [x] 시승 차량 추천 정확도 80% 이상
- [x] 평균 질문 수 3개 이하
- [x] 베이스라인(Random, Rule-based) 대비 15% 이상 성능 향상

### Phase 2 (스케줄링)
- [x] 예약 성사율 80% 이상 (달성: 81%)
- [ ] 대안 수락률 70% 이상
- [ ] 선호 시간 매칭률 60% 이상
- [x] 베이스라인(Random) 대비 20% 성능 향상 (달성: 23.7%)

### 통합 시스템
- [ ] 전체 예약 성공률 75% 이상
- [ ] 총 상호작용 횟수 5회 이하 (질문 + 스케줄링 시도)
- [ ] 개별 Phase 대비 Synergy Bonus 10% 이상
- [ ] End-to-End 처리 시간 3초 이내

---

## 📂 프로젝트 구조

```
driving-test/
├── README.md
├── requirements.txt
├── data/
│   ├── vehicles.json           # 차량 DB (23종)
│   ├── questions.json          # 질문 목록 (8개)
│   ├── centers.json            # 센터 DB
│   └── customer_profiles.json  # 샘플 고객 데이터
├── src/
│   ├── env/
│   │   ├── recommendation_env.py  # Phase 1 환경
│   │   └── scheduling_env.py      # Phase 2 환경
│   ├── agents/
│   │   ├── q_learning_agent.py    # Phase 1 Agent
│   │   └── scheduling_agent.py    # Phase 2 Agent (DQN)
│   ├── baselines/
│   │   ├── random_baseline.py
│   │   ├── rule_based.py
│   │   └── scheduling_baselines.py
│   ├── utils/
│   │   └── customer_generator.py
│   ├── visualization/
│   │   └── plot_results.py
│   ├── train_phase1.py            # Phase 1 학습
│   ├── train_phase2.py            # Phase 2 학습
│   ├── evaluate.py                # 평가 스크립트
│   ├── integrated_system.py       # 통합 시스템 (예정)
│   └── train_integrated.py        # 통합 학습 (예정)
├── models/                        # 학습된 모델 저장
├── results/                       # 실험 결과
└── resource/
    ├── docs/
    │   ├── MDP_DESIGN.md
    │   ├── ROADMAP.md
    │   └── PERFORMANCE_RESULTS.md
    └── image/brand/
```

---

## 📖 참고 자료

- [Gymnasium Documentation](https://gymnasium.farama.org)
- [Spinning Up in Deep RL](https://spinningup.openai.com)
